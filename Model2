import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import joblib
import yaml

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score

# --- 1. CONFIGURATION ---

# Load path from config.yaml
with open('config.yaml', 'r') as f:
    config = yaml.safe_load(f)

# Replaces the hardcoded line 15
INPUT_FILE = config['input_file']

OUTPUT_DIR = os.path.join('files', 'PropensityModel')

# Base columns for TARGET GENERATION and FEATURE ENGINEERING (MUST be original raw columns)
# I have added 'Investable Assets' and kept the other necessary columns.
# Assuming 'Annual Income' is 'Household Annual Income' based on your previous code logic.
CORE_COLS_FOR_ENGINEERING = [
    'Net Worth',
    'Household Annual Income', # Using the full name for consistency with your function logic
    'Market Value of Home',
    'Annualized Premiums',
    'Policy Face Value',
    'Investable Assets' # New column added to the core list
]

# Ensure output directory exists
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

# Plot styling
plt.style.use('ggplot')

def load_data(filepath):
    """Safely loads data."""
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"CRITICAL ERROR: '{filepath}' not found.")
    print(f"READING: Loading '{filepath}'...")
    df = pd.read_excel(filepath)
    return df

def engineer_features(df_core):
    """
    Creates derived features (ratios) based on the CORE_COLS_FOR_ENGINEERING.
    """
    print("    > Engineering features (Ratios & Interactions)...")
    df_eng = df_core.copy()

    # We check the necessary columns needed to calculate the ratios are present
    required_for_ratios = [
        'Policy Face Value', 'Household Annual Income',
        'Annualized Premiums', 'Net Worth', 'Market Value of Home'
    ]
    for col in required_for_ratios:
        if col not in df_core.columns:
            raise KeyError(f"Feature Engineering ERROR: Required column '{col}' is missing for ratio calculation.")

    # 1. Coverage Ratio: Face Value / Income
    df_eng['Ratio_Coverage'] = df_eng['Policy Face Value'] / df_eng['Household Annual Income'].replace(0, 1)

    # 2. Premium Burden: Premiums / Income
    df_eng['Ratio_Premium_Burden'] = df_eng['Annualized Premiums'] / df_eng['Household Annual Income'].replace(0, 1)

    # 3. Asset Wealth: Home Value / Net Worth
    df_eng['Ratio_Asset_Concentration'] = df_eng['Market Value of Home'] / df_eng['Net Worth'].replace(0, 1)

    # Return only the newly engineered features
    return df_eng.loc[:, ['Ratio_Coverage', 'Ratio_Premium_Burden', 'Ratio_Asset_Concentration']]

def generate_propensity_target(df):
    """
    Engineers 'Propensity to Buy' target based on logic: "Underinsured but Wealthy".
    Uses the relevant columns from CORE_COLS_FOR_ENGINEERING.
    """
    print("    > Generating 'Propensity to Buy' labels...")

    # Logic 1: Underinsured Gap (requires Policy Face Value, Household Annual Income)
    ideal_coverage = df['Household Annual Income'] * 10
    coverage_ratio = df['Policy Face Value'] / ideal_coverage.replace(0, 1)

    # Logic 2: Affordability (requires Annualized Premiums, Household Annual Income)
    premium_burden = df['Annualized Premiums'] / df['Household Annual Income'].replace(0, 1)

    # Logic 3: Wealth Rank (requires Net Worth)
    wealth_score = df['Net Worth'].rank(pct=True)

    # --- SCORING ---
    score = (
        (1 - coverage_ratio) * 40 +
        (1 - premium_burden) * 20 +
        (wealth_score) * 30
    )

    # Reduced noise slightly (15 -> 10) to improve learnability
    noise = np.random.normal(0, 10, size=len(df))
    final_score = score + noise

    # Target: Top 25%
    threshold = np.percentile(final_score, 75)
    propensity_target = (final_score > threshold).astype(int)

    print(f"    > Target Distribution: {np.mean(propensity_target)*100:.1f}% Propensity to Buy")
    return propensity_target

def plot_confusion_matrix(y_true, y_pred, title, filename):
    """Generates a robust confusion matrix visualization."""
    cm = confusion_matrix(y_true, y_pred)
    
    # 

    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', cbar=False, annot_kws={"size": 14})
    plt.title(title, fontsize=14)
    plt.ylabel('Actual (0=No, 1=Buy)', fontsize=12)
    plt.xlabel('Predicted (0=No, 1=Buy)', fontsize=12)
    plt.tight_layout()

    save_path = os.path.join(OUTPUT_DIR, filename)
    plt.savefig(save_path)
    plt.close()
    print(f"    > Saved Confusion Matrix: {save_path}")

# --- MAIN PIPELINE ---
if __name__ == "__main__":
    print("--- STARTING PROPENSITY MODEL PIPELINE (Random Forest - Recall Optimized Only) ---")

    # 1. Load Data
    df_raw = load_data(INPUT_FILE)

    # --- IDENTIFY ALL FEATURES ---
    # Get all numerical columns to use as features
    ALL_FEATURE_COLS = df_raw.select_dtypes(include=np.number).columns.tolist()

    # 2. Basic Cleaning & Imputation (for ALL features)
    print("    > Cleaning/Imputing all numerical features...")

    # Check for missing CORE_COLS_FOR_ENGINEERING (critical check)
    missing_core_cols = [col for col in CORE_COLS_FOR_ENGINEERING if col not in df_raw.columns]
    if missing_core_cols:
        raise KeyError(f"CRITICAL ERROR: The following CORE_COLS_FOR_ENGINEERING are missing from the dataset: {missing_core_cols}")

    # Impute NaNs in all numerical columns with the column median
    for col in ALL_FEATURE_COLS:
        df_raw[col] = pd.to_numeric(df_raw[col], errors='coerce').fillna(df_raw[col].median())

    # 3. Generate Target (uses relevant columns from CORE_COLS_FOR_ENGINEERING)
    y = generate_propensity_target(df_raw)

    # 4. Feature Engineering
    df_engineered_ratios = engineer_features(df_raw[CORE_COLS_FOR_ENGINEERING])

    # 5. Combine All Features
    X_raw = df_raw[ALL_FEATURE_COLS]

    # Reset index for clean concat (good practice)
    X_raw = X_raw.reset_index(drop=True)
    df_engineered_ratios = df_engineered_ratios.reset_index(drop=True)

    # Combine all raw features with the engineered ratio features
    X_enhanced = pd.concat([X_raw, df_engineered_ratios], axis=1)

    print(f"    > Training with {X_enhanced.shape[1]} features (All Raw Features + Ratios)")

    # 6. Split Data (80/20)
    X_train, X_test, y_train, y_test = train_test_split(X_enhanced, y, test_size=0.20, random_state=42, stratify=y)

    # 7. Scaling
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # --- MODEL: RANDOM FOREST (Recall Optimized) ---
    print("\n--- MODEL: RANDOM FOREST (Recall Optimized) ---")
    # Uses class_weight='balanced' to force the model to prioritize the minority class (1=Buy)
    rf_rec_model = RandomForestClassifier(n_estimators=100, max_depth=6, random_state=42, class_weight='balanced')
    rf_rec_model.fit(X_train_scaled, y_train)

    y_pred_rf_rec = rf_rec_model.predict(X_test_scaled)
    rf_rec_acc = accuracy_score(y_test, y_pred_rf_rec)
    rf_rec_rec = recall_score(y_test, y_pred_rf_rec)
    print(f"    > Accuracy: {rf_rec_acc:.2%}, Recall: {rf_rec_rec:.2%}")
    print(classification_report(y_test, y_pred_rf_rec))
    plot_confusion_matrix(y_test, y_pred_rf_rec, 'CM: Random Forest (Recall Optimized)', 'propensity_rf_recall_confusion.png')


    # 8. Save Model
    # Since only one model remains, it is automatically the best model.
    best_model_name = 'Random Forest (Recall Optimized)'
    best_model = rf_rec_model

    joblib.dump(best_model, os.path.join(OUTPUT_DIR, 'best_propensity_model.pkl'))
    joblib.dump(scaler, os.path.join(OUTPUT_DIR, 'propensity_scaler.pkl'))

    print("\n--- PIPELINE COMPLETE ---")
    print("----------------------------------------------------------------------")
    print(f"    > FINAL MODEL SAVED: {best_model_name}")
    print(f"    > Final Recall: {rf_rec_rec:.2%}")
    print(f"    > Final Accuracy: {rf_rec_acc:.2%}")
    print(f"    > Output directory: {OUTPUT_DIR}")
    print("----------------------------------------------------------------------")

    # 9. FEATURE IMPORTANCE ANALYSIS
    print("\n--- CALCULATING FEATURE IMPORTANCE ---")

    # Get feature names from the dataframe we created earlier
    feature_names = X_enhanced.columns

    # Create a DataFrame to hold importance data
    importance_df = pd.DataFrame({'Feature': feature_names})

    # Get feature importance (Random Forest logic)
    importance_df['Importance'] = best_model.feature_importances_
    title = f'Feature Importance: {best_model_name}'

    # Sort by absolute value to see the biggest drivers (positive or negative)
    importance_df['Abs_Importance'] = importance_df['Importance'].abs()
    importance_df = importance_df.sort_values(by='Abs_Importance', ascending=False)

    # Display the top drivers in the console
    print(importance_df[['Feature', 'Importance']].head(10))

    # Plotting
    # 
    plt.figure(figsize=(10, 6))
    sns.barplot(x='Importance', y='Feature', data=importance_df.head(15), palette='viridis')

    plt.title(title, fontsize=15)
    plt.xlabel('Impact on Propensity (Importance/Coefficient)', fontsize=12)
    plt.ylabel('Feature', fontsize=12)
    plt.axvline(x=0, color='black', linestyle='--', linewidth=1) # Zero line for reference
    plt.tight_layout()

    # Save the plot
    importance_path = os.path.join(OUTPUT_DIR, 'feature_importance.png')
    plt.savefig(importance_path)
    plt.close()
    print(f"    > Saved Feature Importance Plot: {importance_path}")
